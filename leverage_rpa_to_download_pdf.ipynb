{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "leverage rpa to download pdf.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OpenAIMP/XINN/blob/main/leverage_rpa_to_download_pdf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmsJAA4grCwI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a46eb6cd-3c96-4083-8647-a15f4d6eb10d"
      },
      "source": [
        "pip install sentencepiece "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWCejxr9UZdD",
        "outputId": "f91d8fc4-784e-42e3-e9a1-9a1ce3a6963e"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.10.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.17)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfRoVBAyBTBd",
        "outputId": "1f98b17f-7de0-476d-f6fa-786e813bf4dc"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('words')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqUSPmGjjhoj"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import words\n",
        "from nltk import wordpunct_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ho25__jviIUT"
      },
      "source": [
        "import urllib.request\n",
        "import os\n",
        "pdf_path = \"http://info.kaiserpermanente.org/healthplans/plandocuments/california/pdfs/2021/KPIF-On-Exchange/2021_Kaiser_Permanente-Bronze_60_HDHP_HMO_FINAL.pdf\"\n",
        "filename = os.path.basename(pdf_path)\n",
        "\n",
        "def download_file(download_url, filename):\n",
        "    response = urllib.request.urlopen(download_url)    \n",
        "    file = open(filename , 'wb')\n",
        "    file.write(response.read())\n",
        "    file.close()\n",
        "    return file\n",
        " \n",
        "file_content = download_file(pdf_path, filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhnrmxiE6UQM",
        "outputId": "bc8836ec-c176-4de9-d2ef-11a27f04219c"
      },
      "source": [
        "pip install PyMuPDF"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.7/dist-packages (1.18.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3E4J2Kx6PKG"
      },
      "source": [
        "import fitz\n",
        "\n",
        "def extractText(file): \n",
        "    doc = fitz.open(file) \n",
        "    text = ''\n",
        "    for page in doc: \n",
        "        t = page.getText()\n",
        "        text = text + '\\n' + t\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIXYBpbXjZZG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7ocybp56BBR"
      },
      "source": [
        "def to_sentences(text):\n",
        "  text = text.strip().split('\\n')\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lzna5X329tw8"
      },
      "source": [
        "english_vocabulary = set(nltk.corpus.words.words())\n",
        "#print(english_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJRMhDYe7IAh"
      },
      "source": [
        "def converting_text():\n",
        "  pdf_text = to_sentences(extractText(filename))\n",
        "  text_optional = ''\n",
        "  for x in pdf_text:\n",
        "      splitted_words = x.split(' ')\n",
        "      for word in splitted_words:\n",
        "        if word is not '':\n",
        "          if ((re.search('[$]',word) is not None) and word not in english_vocabulary ):\n",
        "            text_optional =  text_optional + ' '+ word\n",
        "          elif ((re.search('[.]',word) is not None) and word not in english_vocabulary ):\n",
        "            pass\n",
        "          elif word in english_vocabulary:\n",
        "            text_optional =  text_optional + ' '+ word\n",
        "        elif word is '':\n",
        "          pass\n",
        "  new_file = open(\"content.txt\",mode=\"w\",encoding=\"utf-8\")\n",
        "  new_file.write(text_optional)\n",
        "  return text_optional\n",
        "text_optional = converting_text()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyxifZQYoQQp",
        "outputId": "88e4616d-f3c1-480d-b96f-a8c2ed2f4ef4"
      },
      "source": [
        "pip install transformers "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.10.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.17)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4C7bBZhyUrN",
        "outputId": "f86706ac-ddf1-4165-e60a-96f69a151d3b"
      },
      "source": [
        "!apt install libomp-dev"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libomp-dev is already the newest version (5.0.1-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 40 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHaKD7nM6oqp"
      },
      "source": [
        "import torch\n",
        "\n",
        "from transformers import XLMTokenizer, XLMWithLMHeadModel\n",
        "from transformers import LongformerTokenizerFast\n",
        "from transformers import LongformerModel\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRSUOcJwyDJ0",
        "outputId": "60c2f203-645f-454a-8ffb-3d68368e72a9"
      },
      "source": [
        "pip install --upgrade faiss-gpu\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-gpu in /usr/local/lib/python3.7/dist-packages (1.7.1)\n",
            "Collecting faiss-gpu\n",
            "  Using cached faiss_gpu-1.7.1.post2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (89.7 MB)\n",
            "Installing collected packages: faiss-gpu\n",
            "  Attempting uninstall: faiss-gpu\n",
            "    Found existing installation: faiss-gpu 1.7.1\n",
            "    Uninstalling faiss-gpu-1.7.1:\n",
            "      Successfully uninstalled faiss-gpu-1.7.1\n",
            "Successfully installed faiss-gpu-1.7.1.post2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWj87LGGyFqm",
        "outputId": "8a54e192-07be-4ce9-cfae-030e61c509de"
      },
      "source": [
        "pip install --upgrade faiss-gpu==1.7.1\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-gpu==1.7.1\n",
            "  Using cached faiss_gpu-1.7.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (89.7 MB)\n",
            "Installing collected packages: faiss-gpu\n",
            "  Attempting uninstall: faiss-gpu\n",
            "    Found existing installation: faiss-gpu 1.7.1.post2\n",
            "    Uninstalling faiss-gpu-1.7.1.post2:\n",
            "      Successfully uninstalled faiss-gpu-1.7.1.post2\n",
            "Successfully installed faiss-gpu-1.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cY5jeVmXqlns",
        "outputId": "ea71f460-b3b5-4689-d414-36c2c9dba15a"
      },
      "source": [
        "tokenizer = XLMTokenizer.from_pretrained(\"xlm-clm-enfr-1024\")\n",
        "model = XLMWithLMHeadModel.from_pretrained(\"xlm-clm-enfr-1024\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMWithLMHeadModel were not initialized from the model checkpoint at xlm-clm-enfr-1024 and are newly initialized: ['transformer.position_ids']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHUeF12TM9uG"
      },
      "source": [
        "# print(tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tq3U267aNPqN"
      },
      "source": [
        "# translated = model.generate(**tokenizer.prepare_seq2seq_batch(text_optional[0:499], return_tensors=\"pt\"),max_length=500)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgrTM-_TaNMr"
      },
      "source": [
        "# tgt_text = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TpWNzt7ahZb"
      },
      "source": [
        "# print(tgt_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QLmmPH5sr3w"
      },
      "source": [
        "# input_ids = torch.tensor([tokenizer.encode(text_optional[0:499])])\n",
        "# language_id = tokenizer.lang2id['en']\n",
        "# langs = torch.tensor([language_id]*input_ids.shape[1])\n",
        "# print(langs)\n",
        "# langs = langs.view(1, -1)\n",
        "# #print(langs)\n",
        "# output = model(input_ids, langs = langs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-5FhX2ky1xB"
      },
      "source": [
        "# train_data = text_optional[500:999]\n",
        "# "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvpi-Z8geRMV"
      },
      "source": [
        "# from transformers import XLMTokenizer, XLMWithLMHeadModel\n",
        "# import torch\n",
        "\n",
        "# tokenizer = XLMTokenizer.from_pretrained('xlm-mlm-en-2048')\n",
        "# model = XLMWithLMHeadModel.from_pretrained('xlm-mlm-en-2048')\n",
        "\n",
        "# inputs = tokenizer(\"The capital of France is <special1>.\", return_tensors=\"pt\")\n",
        "# labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")[\"input_ids\"]\n",
        "\n",
        "# outputs = model(**inputs, labels=labels)\n",
        "# loss = outputs.loss\n",
        "# logits = outputs.logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWOQVwdY7T13"
      },
      "source": [
        "# translated = model.generate(**tokenizer.prepare_seq2seq_batch(['>>fra<< hi i m good'], return_tensors=\"pt\"))\n",
        "# tgt_text = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0OFBz7P7gfx"
      },
      "source": [
        "# print(tgt_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdNH-DnqeZKg"
      },
      "source": [
        "# print(loss)\n",
        "# print(logits)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXGLxGBu28xY"
      },
      "source": [
        "# logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-I3rOTMNPRp"
      },
      "source": [
        "# print(output.hidden_states)\n",
        "# print(output.attentions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6vj3Tts_geC"
      },
      "source": [
        "#@title Retrieve pipeline of modules and choose English to French translation\n",
        "from transformers import pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdgX3N7x_nuz"
      },
      "source": [
        "# translator = pipeline(\"translation_en_to_fr\")\n",
        "\n",
        "# #One line of code!\n",
        "# print(translator(text_optional[0:400],\n",
        "# max_length=5000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lObkA6N1AOQS"
      },
      "source": [
        "# print(translator(text_optional[0:2000],\n",
        "# max_length=5000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCcuM_Nf_ok6"
      },
      "source": [
        "\n",
        "# # Named entity recognition pipeline, passing in a specific model and tokenizer\n",
        "# tokenizer = XLMTokenizer.from_pretrained('xlm-mlm-en-2048')\n",
        "# model = XLMWithLMHeadModel.from_pretrained('xlm-mlm-en-2048')\n",
        "\n",
        "# t = pipeline('translation_en_to_fr', model=model, tokenizer=tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxdZm8AkRopX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}